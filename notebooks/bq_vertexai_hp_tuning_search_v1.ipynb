{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d4df491-b8e6-48d4-82bd-ac2a5dda4d73",
   "metadata": {},
   "source": [
    "# tl;dr\n",
    "\n",
    "This example runs models locally, but outputs the results of a hyperparameter grid search to a managed `Experiment`. \n",
    "\n",
    "See [here](https://console.cloud.google.com/vertex-ai/locations/us-central1/experiments/search-v1-20211001233901/metrics?project=dataorg-hackweek-2021) for an example. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf5022c-b404-4bad-9ac3-24ad9a4b2e23",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "Install the latest version of Vertex SDK for Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3161201a-fff3-40ee-842e-32218b8a4d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# The Google Cloud Notebook product has specific requirements\n",
    "IS_GOOGLE_CLOUD_NOTEBOOK = os.path.exists(\"/opt/deeplearning/metadata/env_version\")\n",
    "\n",
    "# Google Cloud Notebook requires dependencies to be installed with '--user'\n",
    "USER_FLAG = \"\"\n",
    "if IS_GOOGLE_CLOUD_NOTEBOOK:\n",
    "    USER_FLAG = \"--user\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf96a92-6232-40b8-8f13-c65878aeecd9",
   "metadata": {},
   "source": [
    "Extract project ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d4fdfe9-e3d7-463c-8512-fb695342a065",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project ID:  dataorg-hackweek-2021\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "PROJECT_ID = \"\"\n",
    "\n",
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    # Get your Google Cloud project ID from gcloud\n",
    "    shell_output=!gcloud config list --format 'value(core.project)' 2>/dev/null\n",
    "    PROJECT_ID = shell_output[0]\n",
    "    print(\"Project ID: \", PROJECT_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f6189d-6007-413d-b008-2dc67daa2153",
   "metadata": {},
   "source": [
    "Basically get a timestamp and authentice (which we shouldn't because this is AI notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70b7d1a7-39be-4933-924c-17d89e186bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# If you are running this notebook in Colab, run this cell and follow the\n",
    "# instructions to authenticate your GCP account. This provides access to your\n",
    "# Cloud Storage bucket and lets you submit training jobs and prediction\n",
    "# requests.\n",
    "\n",
    "# The Google Cloud Notebook product has specific requirements\n",
    "IS_GOOGLE_CLOUD_NOTEBOOK = os.path.exists(\"/opt/deeplearning/metadata/env_version\")\n",
    "\n",
    "# If on Google Cloud Notebooks, then don't execute this code\n",
    "if not IS_GOOGLE_CLOUD_NOTEBOOK:\n",
    "    if \"google.colab\" in sys.modules:\n",
    "        from google.colab import auth as google_auth\n",
    "\n",
    "        google_auth.authenticate_user()\n",
    "\n",
    "    # If you are running this notebook locally, replace the string below with the\n",
    "    # path to your service account key and run this cell to authenticate your GCP\n",
    "    # account.\n",
    "    elif not os.getenv(\"IS_TESTING\"):\n",
    "        %env GOOGLE_APPLICATION_CREDENTIALS ''\n",
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce7d110-5192-441a-b1ca-79c3a5fbdea1",
   "metadata": {},
   "source": [
    "Bucket to save things off (assummed to exist)\n",
    "see `bq_vertexai_train_deploy_search_v1.ipynb` on how to create"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d8f24b82-14f5-4a60-9250-e7e9e837baa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET_NAME = \"gs://tmp-hackweek2021-model-training\"  # @param {type:\"string\"}\n",
    "REGION = \"us-central1\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d50ab2-266d-4ac7-8672-b4565d023475",
   "metadata": {},
   "source": [
    "List the items in the bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35a53720-8dd0-4425-83a5-74fb5b854913",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      4139  2021-09-30T17:05:30Z  gs://tmp-hackweek2021-model-training/aiplatform-2021-09-30-17:05:30.399-aiplatform_custom_trainer_script-0.1.tar.gz#1633021530511685  metageneration=1\n",
      "      4539  2021-09-30T23:00:27Z  gs://tmp-hackweek2021-model-training/aiplatform-2021-09-30-23:00:27.039-aiplatform_custom_trainer_script-0.1.tar.gz#1633042827164033  metageneration=1\n",
      "      4133  2021-09-30T23:10:45Z  gs://tmp-hackweek2021-model-training/aiplatform-2021-09-30-23:10:44.922-aiplatform_custom_trainer_script-0.1.tar.gz#1633043445045222  metageneration=1\n",
      "      4533  2021-09-30T23:15:59Z  gs://tmp-hackweek2021-model-training/aiplatform-2021-09-30-23:15:59.246-aiplatform_custom_trainer_script-0.1.tar.gz#1633043759351736  metageneration=1\n",
      "      4361  2021-09-30T23:26:56Z  gs://tmp-hackweek2021-model-training/aiplatform-2021-09-30-23:26:56.424-aiplatform_custom_trainer_script-0.1.tar.gz#1633044416509545  metageneration=1\n",
      "      2067  2021-09-30T23:42:57Z  gs://tmp-hackweek2021-model-training/aiplatform-2021-09-30-23:42:57.781-aiplatform_custom_trainer_script-0.1.tar.gz#1633045377877340  metageneration=1\n",
      "      2223  2021-10-01T22:44:02Z  gs://tmp-hackweek2021-model-training/aiplatform-2021-10-01-22:44:02.433-aiplatform_custom_trainer_script-0.1.tar.gz#1633128242542918  metageneration=1\n",
      "      2226  2021-10-01T23:28:38Z  gs://tmp-hackweek2021-model-training/aiplatform-2021-10-01-23:28:38.561-aiplatform_custom_trainer_script-0.1.tar.gz#1633130918645564  metageneration=1\n",
      "       297  2021-09-30T17:04:29Z  gs://tmp-hackweek2021-model-training/mean_and_std.json#1633021469934116  metageneration=1\n",
      "                                 gs://tmp-hackweek2021-model-training/aiplatform-custom-training-2021-09-30-17:05:30.544/\n",
      "                                 gs://tmp-hackweek2021-model-training/aiplatform-custom-training-2021-09-30-23:10:45.100/\n",
      "                                 gs://tmp-hackweek2021-model-training/aiplatform-custom-training-2021-09-30-23:42:57.906/\n",
      "TOTAL: 9 objects, 28518 bytes (27.85 KiB)\n"
     ]
    }
   ],
   "source": [
    "! gsutil ls -al $BUCKET_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea0de419-1fb0-4b93-9284-1267a9b84939",
   "metadata": {},
   "source": [
    "## AI Platform Config\n",
    "\n",
    "Initialize Vertex AI and the experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "109d0b66-f742-483f-85d0-6222acf2a1f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:root:Resource search-v1-20211001233901 not found.\n",
      "INFO:root:Creating Resource search-v1-20211001233901\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "from google.cloud import aiplatform\n",
    "from google.cloud.aiplatform import gapic as aip\n",
    "\n",
    "EXPERIMENT_NAME = \"search-v1-\" + TIMESTAMP\n",
    "\n",
    "aiplatform.init(project=PROJECT_ID, location=REGION, \n",
    "                staging_bucket=BUCKET_NAME,\n",
    "                experiment=EXPERIMENT_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f6e172-f8d1-41da-a068-b7379ce240ac",
   "metadata": {},
   "source": [
    "## Model Train Config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9aa4ca2-0635-46d2-b24a-29fe2704f3ed",
   "metadata": {},
   "source": [
    "## Custom Python Training Code\n",
    "\n",
    "This utilize train/test/validation splits the `bq_vertexai_train_deploy_search_v1.ipynb` example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5b8764cb-a961-4489-90c2-ae4277b6eac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras.layers import Dense, LSTM, Embedding, Dropout\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from google.cloud import bigquery\n",
    "from google.cloud import storage\n",
    "\n",
    "# Utilizes previously created tables from the other example\n",
    "training_data_uri = 'bq://dataorg-hackweek-2021.dataset_3809367985592729600_tables_2021_09_30T16_00_27_766Z.training'\n",
    "validation_data_uri = 'bq://dataorg-hackweek-2021.dataset_3809367985592729600_tables_2021_09_30T16_00_27_766Z.validation'\n",
    "test_data_uri = 'bq://dataorg-hackweek-2021.dataset_3809367985592729600_tables_2021_09_30T16_00_27_766Z.test'\n",
    "\n",
    "\n",
    "# Set up BigQuery clients\n",
    "bqclient = bigquery.Client()\n",
    "\n",
    "# Download a table\n",
    "def download_table(bq_table_uri: str):\n",
    "    # Remove bq:// prefix if present\n",
    "    prefix = \"bq://\"\n",
    "    if bq_table_uri.startswith(prefix):\n",
    "        bq_table_uri = bq_table_uri[len(prefix):]\n",
    "\n",
    "    table = bigquery.TableReference.from_string(bq_table_uri)\n",
    "    rows = bqclient.list_rows(\n",
    "        table,\n",
    "    )\n",
    "    return rows.to_dataframe(create_bqstorage_client=False)\n",
    "\n",
    "\n",
    "df_train = download_table(training_data_uri)\n",
    "df_validation = download_table(validation_data_uri)\n",
    "df_test = download_table(test_data_uri)\n",
    "\n",
    "df_train = df_train[~df_train.title.isnull()]\n",
    "df_validation = df_validation[~df_validation.title.isnull()]\n",
    "df_test = df_test[~df_test.title.isnull()]\n",
    "\n",
    "# create the model\n",
    "max_tokens = 1000\n",
    "max_len = 100\n",
    "vectorize_layer = TextVectorization(\n",
    "  max_tokens=max_tokens,\n",
    "  output_mode=\"int\",\n",
    "  output_sequence_length=max_len,\n",
    ")\n",
    "\n",
    "train_texts = df_train[\"title\"].to_numpy()\n",
    "vectorize_layer.adapt(train_texts)\n",
    "\n",
    "# format the labels\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(sorted(df_train[\"selected\"].unique()))\n",
    "labels = encoder.transform(df_train['selected'].tolist())\n",
    "val_labels = encoder.transform(df_validation['selected'].tolist())\n",
    "test_labels = encoder.transform(df_test['selected'].tolist())\n",
    "\n",
    "def train(X_train, y_train, X_validation, y_validation, epochs, nunits):\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Input(shape=(1,), dtype=\"string\"))\n",
    "    model.add(vectorize_layer)\n",
    "    model.add(Embedding(max_tokens + 1, 128))\n",
    "    model.add(LSTM(nunits))\n",
    "    model.add(Dense(nunits, activation=\"relu\"))\n",
    "    model.add(Dense(df_train[\"labels\"].unique().size, activation=\"sigmoid\"))\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    # train the model\n",
    "    history = model.fit(X_train, y_train, epochs=epochs, \n",
    "                        validation_data=(X_validation, y_validation))\n",
    "    return(model, history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "720a5349-45bf-4c5a-a75c-5b8ff5f7bc15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "39/39 [==============================] - 5s 139ms/step - loss: 4.7242 - accuracy: 0.7693 - val_loss: 4.3367 - val_accuracy: 0.8630\n",
      "Epoch 2/3\n",
      "39/39 [==============================] - 5s 122ms/step - loss: 4.1472 - accuracy: 0.8645 - val_loss: 4.0492 - val_accuracy: 0.8630\n",
      "Epoch 3/3\n",
      "39/39 [==============================] - 5s 124ms/step - loss: 4.0380 - accuracy: 0.8645 - val_loss: 4.0326 - val_accuracy: 0.8630\n",
      "5/5 - 0s - loss: 4.0327 - accuracy: 0.7895\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 3, got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_31731/2167902373.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m     )\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmae\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'title'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m     \u001b[0maiplatform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"eval_loss\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"eval_mae\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmae\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"eval_mse\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmse\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 3, got 2)"
     ]
    }
   ],
   "source": [
    "parameters = [\n",
    "    {\"num_units\": 32, \"epochs\": 3},\n",
    "    {\"num_units\": 32, \"epochs\": 10},\n",
    "    {\"num_units\": 32, \"epochs\": 20},\n",
    "    {\"num_units\": 64, \"epochs\": 3},\n",
    "    {\"num_units\": 64, \"epochs\": 10},\n",
    "    {\"num_units\": 64, \"epochs\": 20}\n",
    "]\n",
    "\n",
    "for i, params in enumerate(parameters):\n",
    "    aiplatform.start_run(run=f\"search-v1-local-run-{i}\")\n",
    "    aiplatform.log_params(params)\n",
    "    model, history = train(\n",
    "        df_train[\"title\"].to_numpy(), \n",
    "        labels, \n",
    "        df_validation[\"title\"].to_numpy(), \n",
    "        val_labels,               \n",
    "        epochs=params[\"epochs\"],\n",
    "        nunits=params[\"num_units\"]\n",
    "    )\n",
    "    aiplatform.log_metrics(\n",
    "        {metric: values[-1] for metric, values in history.history.items()}\n",
    "    )\n",
    "\n",
    "    loss, accuracy = model.evaluate(df_test['title'].to_numpy(), test_labels, verbose=2)\n",
    "    aiplatform.log_metrics({\"eval_loss\": loss, \"eval_accuracy\": mae, \"eval_mse\": mse})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "eae0e9d7-eee8-47f7-94e3-eaaf4408fde1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 - 0s - loss: 4.0327 - accuracy: 0.7895\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[4.032702922821045, 0.7894737124443054]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(df_test['title'].to_numpy(), test_labels, verbose=2)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-3.m80",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-3:m80"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
