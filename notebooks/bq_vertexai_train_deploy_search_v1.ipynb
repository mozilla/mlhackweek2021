{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dbf5022c-b404-4bad-9ac3-24ad9a4b2e23",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "Install the latest version of Vertex SDK for Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3161201a-fff3-40ee-842e-32218b8a4d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# The Google Cloud Notebook product has specific requirements\n",
    "IS_GOOGLE_CLOUD_NOTEBOOK = os.path.exists(\"/opt/deeplearning/metadata/env_version\")\n",
    "\n",
    "# Google Cloud Notebook requires dependencies to be installed with '--user'\n",
    "USER_FLAG = \"\"\n",
    "if IS_GOOGLE_CLOUD_NOTEBOOK:\n",
    "    USER_FLAG = \"--user\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a17e01d6-dd9c-41d1-8255-d613fb45d530",
   "metadata": {},
   "source": [
    "Install pandas BigQuery connector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7876096b-7459-4a0a-a8d8-f62aa7617432",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas-gbq\n",
      "  Downloading pandas_gbq-0.15.0-py3-none-any.whl (25 kB)\n",
      "Requirement already satisfied: google-auth in /opt/conda/lib/python3.7/site-packages (from pandas-gbq) (1.35.0)\n",
      "Requirement already satisfied: pandas>=0.23.2 in /opt/conda/lib/python3.7/site-packages (from pandas-gbq) (1.3.3)\n",
      "Requirement already satisfied: google-cloud-bigquery[bqstorage,pandas]!=2.4.*,<3.0.0dev,>=1.11.1 in ./.local/lib/python3.7/site-packages (from pandas-gbq) (2.27.1)\n",
      "Requirement already satisfied: google-auth-oauthlib in /opt/conda/lib/python3.7/site-packages (from pandas-gbq) (0.4.6)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from pandas-gbq) (58.0.4)\n",
      "Collecting pydata-google-auth\n",
      "  Downloading pydata_google_auth-1.2.0-py2.py3-none-any.whl (13 kB)\n",
      "Requirement already satisfied: proto-plus>=1.10.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery[bqstorage,pandas]!=2.4.*,<3.0.0dev,>=1.11.1->pandas-gbq) (1.19.0)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery[bqstorage,pandas]!=2.4.*,<3.0.0dev,>=1.11.1->pandas-gbq) (2.25.1)\n",
      "Requirement already satisfied: packaging>=14.3 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery[bqstorage,pandas]!=2.4.*,<3.0.0dev,>=1.11.1->pandas-gbq) (21.0)\n",
      "Requirement already satisfied: protobuf>=3.12.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery[bqstorage,pandas]!=2.4.*,<3.0.0dev,>=1.11.1->pandas-gbq) (3.18.0)\n",
      "Requirement already satisfied: google-cloud-core<3.0.0dev,>=1.4.1 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery[bqstorage,pandas]!=2.4.*,<3.0.0dev,>=1.11.1->pandas-gbq) (2.0.0)\n",
      "Requirement already satisfied: google-resumable-media<3.0dev,>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery[bqstorage,pandas]!=2.4.*,<3.0.0dev,>=1.11.1->pandas-gbq) (2.0.3)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.38.1 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery[bqstorage,pandas]!=2.4.*,<3.0.0dev,>=1.11.1->pandas-gbq) (1.38.1)\n",
      "Requirement already satisfied: google-api-core[grpc]<3.0.0dev,>=1.29.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery[bqstorage,pandas]!=2.4.*,<3.0.0dev,>=1.11.1->pandas-gbq) (1.31.2)\n",
      "Requirement already satisfied: pyarrow<6.0dev,>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery[bqstorage,pandas]!=2.4.*,<3.0.0dev,>=1.11.1->pandas-gbq) (5.0.0)\n",
      "Requirement already satisfied: google-cloud-bigquery-storage<3.0.0dev,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery[bqstorage,pandas]!=2.4.*,<3.0.0dev,>=1.11.1->pandas-gbq) (2.8.0)\n",
      "Requirement already satisfied: pytz in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]<3.0.0dev,>=1.29.0->google-cloud-bigquery[bqstorage,pandas]!=2.4.*,<3.0.0dev,>=1.11.1->pandas-gbq) (2021.1)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]<3.0.0dev,>=1.29.0->google-cloud-bigquery[bqstorage,pandas]!=2.4.*,<3.0.0dev,>=1.11.1->pandas-gbq) (1.53.0)\n",
      "Requirement already satisfied: six>=1.13.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]<3.0.0dev,>=1.29.0->google-cloud-bigquery[bqstorage,pandas]!=2.4.*,<3.0.0dev,>=1.11.1->pandas-gbq) (1.16.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth->pandas-gbq) (0.2.7)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth->pandas-gbq) (4.7.2)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth->pandas-gbq) (4.2.2)\n",
      "Requirement already satisfied: libcst>=0.2.5 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery-storage<3.0.0dev,>=2.0.0->google-cloud-bigquery[bqstorage,pandas]!=2.4.*,<3.0.0dev,>=1.11.1->pandas-gbq) (0.3.21)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /opt/conda/lib/python3.7/site-packages (from google-resumable-media<3.0dev,>=0.6.0->google-cloud-bigquery[bqstorage,pandas]!=2.4.*,<3.0.0dev,>=1.11.1->pandas-gbq) (1.2.0)\n",
      "Requirement already satisfied: typing-inspect>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from libcst>=0.2.5->google-cloud-bigquery-storage<3.0.0dev,>=2.0.0->google-cloud-bigquery[bqstorage,pandas]!=2.4.*,<3.0.0dev,>=1.11.1->pandas-gbq) (0.7.1)\n",
      "Requirement already satisfied: pyyaml>=5.2 in /opt/conda/lib/python3.7/site-packages (from libcst>=0.2.5->google-cloud-bigquery-storage<3.0.0dev,>=2.0.0->google-cloud-bigquery[bqstorage,pandas]!=2.4.*,<3.0.0dev,>=1.11.1->pandas-gbq) (5.4.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.2 in /opt/conda/lib/python3.7/site-packages (from libcst>=0.2.5->google-cloud-bigquery-storage<3.0.0dev,>=2.0.0->google-cloud-bigquery[bqstorage,pandas]!=2.4.*,<3.0.0dev,>=1.11.1->pandas-gbq) (3.10.0.2)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=14.3->google-cloud-bigquery[bqstorage,pandas]!=2.4.*,<3.0.0dev,>=1.11.1->pandas-gbq) (2.4.7)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /opt/conda/lib/python3.7/site-packages (from pandas>=0.23.2->pandas-gbq) (1.19.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas>=0.23.2->pandas-gbq) (2.8.2)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth->pandas-gbq) (0.4.8)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-bigquery[bqstorage,pandas]!=2.4.*,<3.0.0dev,>=1.11.1->pandas-gbq) (4.0.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-bigquery[bqstorage,pandas]!=2.4.*,<3.0.0dev,>=1.11.1->pandas-gbq) (1.26.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-bigquery[bqstorage,pandas]!=2.4.*,<3.0.0dev,>=1.11.1->pandas-gbq) (2021.5.30)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-bigquery[bqstorage,pandas]!=2.4.*,<3.0.0dev,>=1.11.1->pandas-gbq) (2.10)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /opt/conda/lib/python3.7/site-packages (from typing-inspect>=0.4.0->libcst>=0.2.5->google-cloud-bigquery-storage<3.0.0dev,>=2.0.0->google-cloud-bigquery[bqstorage,pandas]!=2.4.*,<3.0.0dev,>=1.11.1->pandas-gbq) (0.4.3)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from google-auth-oauthlib->pandas-gbq) (1.3.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib->pandas-gbq) (3.1.1)\n",
      "Installing collected packages: pydata-google-auth, pandas-gbq\n",
      "Successfully installed pandas-gbq-0.15.0 pydata-google-auth-1.2.0\n"
     ]
    }
   ],
   "source": [
    "! pip install {USER_FLAG} -U \"pandas-gbq\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2198ecb-0b9f-4b0c-97e3-149d5edcc31c",
   "metadata": {},
   "source": [
    "### Restart the kernel\n",
    "\n",
    "Once you've installed everything, you need to restart the notebook kernel so it can find the packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee0dc9f0-accf-4e15-9486-969d22a4fe4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    # Automatically restart kernel after installs\n",
    "    import IPython\n",
    "\n",
    "    app = IPython.Application.instance()\n",
    "    app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf96a92-6232-40b8-8f13-c65878aeecd9",
   "metadata": {},
   "source": [
    "Extract project ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d4fdfe9-e3d7-463c-8512-fb695342a065",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project ID:  dataorg-hackweek-2021\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "PROJECT_ID = \"\"\n",
    "\n",
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    # Get your Google Cloud project ID from gcloud\n",
    "    shell_output=!gcloud config list --format 'value(core.project)' 2>/dev/null\n",
    "    PROJECT_ID = shell_output[0]\n",
    "    print(\"Project ID: \", PROJECT_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f6189d-6007-413d-b008-2dc67daa2153",
   "metadata": {},
   "source": [
    "Basically get a timestamp and authentice (which we shouldn't because this is AI notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70b7d1a7-39be-4933-924c-17d89e186bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# If you are running this notebook in Colab, run this cell and follow the\n",
    "# instructions to authenticate your GCP account. This provides access to your\n",
    "# Cloud Storage bucket and lets you submit training jobs and prediction\n",
    "# requests.\n",
    "\n",
    "# The Google Cloud Notebook product has specific requirements\n",
    "IS_GOOGLE_CLOUD_NOTEBOOK = os.path.exists(\"/opt/deeplearning/metadata/env_version\")\n",
    "\n",
    "# If on Google Cloud Notebooks, then don't execute this code\n",
    "if not IS_GOOGLE_CLOUD_NOTEBOOK:\n",
    "    if \"google.colab\" in sys.modules:\n",
    "        from google.colab import auth as google_auth\n",
    "\n",
    "        google_auth.authenticate_user()\n",
    "\n",
    "    # If you are running this notebook locally, replace the string below with the\n",
    "    # path to your service account key and run this cell to authenticate your GCP\n",
    "    # account.\n",
    "    elif not os.getenv(\"IS_TESTING\"):\n",
    "        %env GOOGLE_APPLICATION_CREDENTIALS ''\n",
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce7d110-5192-441a-b1ca-79c3a5fbdea1",
   "metadata": {},
   "source": [
    "Set up a bucket to save things off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d8f24b82-14f5-4a60-9250-e7e9e837baa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET_NAME = \"gs://tmp-hackweek2021-model-training\"  # @param {type:\"string\"}\n",
    "REGION = \"us-central1\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3749827-fb64-4baf-8e70-3d0a46024c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment to create the bucket\n",
    "# ! gsutil mb -l $REGION $BUCKET_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d50ab2-266d-4ac7-8672-b4565d023475",
   "metadata": {},
   "source": [
    "List the items in the bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35a53720-8dd0-4425-83a5-74fb5b854913",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      4139  2021-09-30T17:05:30Z  gs://tmp-hackweek2021-model-training/aiplatform-2021-09-30-17:05:30.399-aiplatform_custom_trainer_script-0.1.tar.gz#1633021530511685  metageneration=1\n",
      "       297  2021-09-30T17:04:29Z  gs://tmp-hackweek2021-model-training/mean_and_std.json#1633021469934116  metageneration=1\n",
      "                                 gs://tmp-hackweek2021-model-training/aiplatform-custom-training-2021-09-30-17:05:30.544/\n",
      "TOTAL: 2 objects, 4436 bytes (4.33 KiB)\n"
     ]
    }
   ],
   "source": [
    "! gsutil ls -al $BUCK`ET_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea0de419-1fb0-4b93-9284-1267a9b84939",
   "metadata": {},
   "source": [
    "## AI Platform Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "109d0b66-f742-483f-85d0-6222acf2a1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "from google.cloud import aiplatform\n",
    "from google.cloud.aiplatform import gapic as aip\n",
    "\n",
    "aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=BUCKET_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "45f3e40c-b30c-4990-b0b1-639141baca86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<AcceleratorType.NVIDIA_TESLA_K80: 1>, 4)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(aip.AcceleratorType.NVIDIA_TESLA_K80, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f75023a8-932b-404e-8570-31ee2e27b563",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_GPU, TRAIN_NGPU = (aip.AcceleratorType.NVIDIA_TESLA_K80, 1)\n",
    "\n",
    "DEPLOY_GPU, DEPLOY_NGPU = (aip.AcceleratorType.NVIDIA_TESLA_K80, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8a380993-ef65-47da-93f4-9b4ecfa29bd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: us-docker.pkg.dev/vertex-ai/training/tf-gpu.2-4:latest AcceleratorType.NVIDIA_TESLA_K80 1\n",
      "Deployment: us-docker.pkg.dev/vertex-ai/prediction/tf2-gpu.2-4:latest AcceleratorType.NVIDIA_TESLA_K80 1\n"
     ]
    }
   ],
   "source": [
    "TRAIN_VERSION = \"tf-gpu.2-4\"\n",
    "DEPLOY_VERSION = \"tf2-gpu.2-4\"\n",
    "\n",
    "TRAIN_IMAGE = \"us-docker.pkg.dev/vertex-ai/training/{}:latest\".format(TRAIN_VERSION)\n",
    "DEPLOY_IMAGE = \"us-docker.pkg.dev/vertex-ai/prediction/{}:latest\".format(DEPLOY_VERSION)\n",
    "\n",
    "print(\"Training:\", TRAIN_IMAGE, TRAIN_GPU, TRAIN_NGPU)\n",
    "print(\"Deployment:\", DEPLOY_IMAGE, DEPLOY_GPU, DEPLOY_NGPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c6989b47-4e37-4e15-8c95-91057c7f8416",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train machine type n1-standard-4\n",
      "Deploy machine type n1-standard-4\n"
     ]
    }
   ],
   "source": [
    "MACHINE_TYPE = \"n1-standard\"\n",
    "\n",
    "VCPU = \"4\"\n",
    "TRAIN_COMPUTE = MACHINE_TYPE + \"-\" + VCPU\n",
    "print(\"Train machine type\", TRAIN_COMPUTE)\n",
    "\n",
    "MACHINE_TYPE = \"n1-standard\"\n",
    "\n",
    "VCPU = \"4\"\n",
    "DEPLOY_COMPUTE = MACHINE_TYPE + \"-\" + VCPU\n",
    "print(\"Deploy machine type\", DEPLOY_COMPUTE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f6e172-f8d1-41da-a068-b7379ce240ac",
   "metadata": {},
   "source": [
    "## Model Train Config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25bd59a5-524e-45bb-bec6-6f4f0bc54aeb",
   "metadata": {},
   "source": [
    "Query the data to flatten it out and export to a new table.\n",
    "\n",
    "This makes the train-test-validation split conceptual easier when training the model. \n",
    "e.g., Can easily see the tables that were directly trained on in BQ Console"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "df6f5141-0c7c-4099-bf89-f40a139af9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "BQ_SOURCE_RAW = \"bq://moz-fx-data-shared-prod.mlhackweek_search_stable.action_v1\"\n",
    "BQ_SOURCE_TRANSFORMED = \"bq://mozdata.analysis.ccd_action_transformed_v1_tmp\" # required to write to mozdata as credentials won't write to mlhackweek_search dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5e9496bb-eeef-481c-9471-32dca6f40533",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1516 out of 1516 rows loaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:05,  5.04s/it]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "import numpy as np\n",
    "# Calculate mean and std across all rows\n",
    "from google.cloud import bigquery\n",
    "\n",
    "os.environ[\"GCLOUD_PROJECT\"] = PROJECT_ID\n",
    "\n",
    "NA_VALUES = [\"NA\", \".\"]\n",
    "\n",
    "# Set up BigQuery clients\n",
    "bqclient = bigquery.Client()\n",
    "\n",
    "query = '''\n",
    "select \n",
    "    metrics.string.search_meta_search_text as query,\n",
    "    CAST(SPLIT(u.key, \"_\")[OFFSET(1)] AS INT64) as position,\n",
    "    SPLIT(u.value, \"/\")[OFFSET(2)] as domain,\n",
    "    p.value as preamble,\n",
    "    d.value as short_description,\n",
    "    t.value as title,\n",
    "    s.value as selected\n",
    "from `{tbl}`\n",
    "cross join unnest(metrics.labeled_boolean.search_meta_selected) s\n",
    "cross join unnest(metrics.labeled_string.search_meta_url) u\n",
    "cross join unnest(metrics.labeled_string.search_meta_preamble) p\n",
    "cross join unnest(metrics.labeled_string.search_meta_short_description) d\n",
    "cross join unnest(metrics.labeled_string.search_meta_title) t\n",
    "where submission_timestamp >=\"2021-01-01\"\n",
    "and CAST(SPLIT(s.key, \"_\")[OFFSET(1)] AS INT64) = CAST(SPLIT(u.key, \"_\")[OFFSET(1)] AS INT64)\n",
    "and CAST(SPLIT(s.key, \"_\")[OFFSET(1)] AS INT64) = CAST(SPLIT(p.key, \"_\")[OFFSET(1)] AS INT64)\n",
    "and CAST(SPLIT(s.key, \"_\")[OFFSET(1)] AS INT64) = CAST(SPLIT(d.key, \"_\")[OFFSET(2)] AS INT64)\n",
    "and CAST(SPLIT(s.key, \"_\")[OFFSET(1)] AS INT64) = CAST(SPLIT(t.key, \"_\")[OFFSET(1)] AS INT64)\n",
    "'''\n",
    "\n",
    "\n",
    "df = bqclient.query(query.format(tbl = BQ_SOURCE_RAW[5:])).to_dataframe()\n",
    "df.to_gbq(BQ_SOURCE_TRANSFORMED[5:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b3d75b23-9e81-470d-97ec-0d55fd5e2fd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.datasets.dataset:Creating TabularDataset\n",
      "INFO:google.cloud.aiplatform.datasets.dataset:Create TabularDataset backing LRO: projects/424090863877/locations/us-central1/datasets/3809367985592729600/operations/198032764400828416\n",
      "INFO:google.cloud.aiplatform.datasets.dataset:TabularDataset created. Resource name: projects/424090863877/locations/us-central1/datasets/3809367985592729600\n",
      "INFO:google.cloud.aiplatform.datasets.dataset:To use this TabularDataset in another session:\n",
      "INFO:google.cloud.aiplatform.datasets.dataset:ds = aiplatform.TabularDataset('projects/424090863877/locations/us-central1/datasets/3809367985592729600')\n"
     ]
    }
   ],
   "source": [
    "dataset = aiplatform.TabularDataset.create(\n",
    "    display_name=\"search_v1_transformed\", bq_source=BQ_SOURCE_TRANSFORMED\n",
    "    # display_name=\"search_v1\", bq_source=BQ_SOURCE_RAW\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ba44f869-6290-4dc2-83bb-083426593345",
   "metadata": {},
   "outputs": [],
   "source": [
    "JOB_NAME = \"ccd_job_v1_\" + TIMESTAMP\n",
    "\n",
    "if not TRAIN_NGPU or TRAIN_NGPU < 2:\n",
    "    TRAIN_STRATEGY = \"single\"\n",
    "else:\n",
    "    TRAIN_STRATEGY = \"mirror\"\n",
    "\n",
    "EPOCHS = 20\n",
    "BATCH_SIZE = 10\n",
    "\n",
    "CMDARGS = [\n",
    "    \"--epochs=\" + str(EPOCHS),\n",
    "    \"--batch_size=\" + str(BATCH_SIZE),\n",
    "    \"--distribute=\" + TRAIN_STRATEGY\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9aa4ca2-0635-46d2-b24a-29fe2704f3ed",
   "metadata": {},
   "source": [
    "## Custom Python Training Code\n",
    "\n",
    "This is written to docker container and ran\n",
    "\n",
    "* The training/validation/test splits are given by environmental variables that define the BQ Tables\n",
    "  - These splits are created when the job is ran.\n",
    "  - These are saved off in the running project BQ table. A new dataset is created with three tables (e.g., training, test, split).\n",
    "  - This dataset can be found by going to the Vertex AI console: Training -> Custom Jobs -> View Custom Job Inputs in JSON. \n",
    "       - (not user friendly, no intuitive).\n",
    "  - e.g., `dataorg-hackweek-2021.dataset_3809367985592729600_tables_2021_09_30T16_00_27_766Z.train`\n",
    "  \n",
    "  \n",
    "**WARNING** \n",
    "\n",
    "This takes around 15 minutes to run.\n",
    "It also depends when Google gets around to provisioning the resources. \n",
    "Going to Training -> Custom Jobs -> [View Logs](https://console.cloud.google.com/logs/query;query=resource.labels.job_id%3D%228878752107360944128%22%20timestamp%3E%3D%222021-09-30T23:47:01.947333Z%22;cursorTimestamp=2021-09-30T23:57:49.341720103Z?project=dataorg-hackweek-2021) gives the output from running the job "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5b8764cb-a961-4489-90c2-ae4277b6eac2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing task_search_v1.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile task_search_v1.py\n",
    "\n",
    "import argparse\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras.layers import Dense, LSTM, Embedding, Dropout\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from google.cloud import bigquery\n",
    "from google.cloud import storage\n",
    "\n",
    "# Read environmental variables\n",
    "training_data_uri = os.environ[\"AIP_TRAINING_DATA_URI\"]\n",
    "validation_data_uri = os.environ[\"AIP_VALIDATION_DATA_URI\"]\n",
    "test_data_uri = os.environ[\"AIP_TEST_DATA_URI\"]\n",
    "\n",
    "# Read args\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--epochs', dest='epochs',\n",
    "                    default=10, type=int,\n",
    "                    help='Number of epochs.')\n",
    "parser.add_argument('--batch_size', dest='batch_size',\n",
    "                    default=10, type=int,\n",
    "                    help='Batch size.')\n",
    "parser.add_argument('--distribute', dest='distribute', type=str, default='single',\n",
    "                    help='Distributed training strategy.')\n",
    "args = parser.parse_args()\n",
    "\n",
    "# Single Machine, single compute device\n",
    "if args.distribute == 'single':\n",
    "    if tf.test.is_gpu_available():\n",
    "        strategy = tf.distribute.OneDeviceStrategy(device=\"/gpu:0\")\n",
    "    else:\n",
    "        strategy = tf.distribute.OneDeviceStrategy(device=\"/cpu:0\")\n",
    "# Single Machine, multiple compute device\n",
    "elif args.distribute == 'mirror':\n",
    "    strategy = tf.distribute.MirroredStrategy()\n",
    "# Multiple Machine, multiple compute device\n",
    "elif args.distribute == 'multi':\n",
    "    strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()\n",
    "\n",
    "# Set up BigQuery clients\n",
    "bqclient = bigquery.Client()\n",
    "\n",
    "# Download a table\n",
    "def download_table(bq_table_uri: str):\n",
    "    # Remove bq:// prefix if present\n",
    "    prefix = \"bq://\"\n",
    "    if bq_table_uri.startswith(prefix):\n",
    "        bq_table_uri = bq_table_uri[len(prefix):]\n",
    "\n",
    "    table = bigquery.TableReference.from_string(bq_table_uri)\n",
    "    rows = bqclient.list_rows(\n",
    "        table,\n",
    "    )\n",
    "    return rows.to_dataframe(create_bqstorage_client=False)\n",
    "\n",
    "\n",
    "df_train = download_table(training_data_uri)\n",
    "\n",
    "df_train = df_train[~df_train.title.isnull()]\n",
    "\n",
    "# df_validation = download_table(validation_data_uri)\n",
    "# df_test = download_table(test_data_uri)\n",
    "\n",
    "# create the model\n",
    "max_tokens = 1000\n",
    "max_len = 100\n",
    "vectorize_layer = TextVectorization(\n",
    "  max_tokens=max_tokens,\n",
    "  output_mode=\"int\",\n",
    "  output_sequence_length=max_len,\n",
    ")\n",
    "\n",
    "train_texts = df_train[\"title\"].to_numpy()\n",
    "vectorize_layer.adapt(train_texts)\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Input(shape=(1,), dtype=\"string\"))\n",
    "model.add(vectorize_layer)\n",
    "model.add(Embedding(max_tokens + 1, 128))\n",
    "model.add(LSTM(64))\n",
    "model.add(Dense(64, activation=\"relu\"))\n",
    "model.add(Dense(df_train[\"domain\"].unique().size, activation=\"sigmoid\"))\n",
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# format the labels\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(sorted(df_train[\"domain\"].unique()))\n",
    "labels = encoder.transform(df_train['domain'].tolist())\n",
    "\n",
    "# train the model\n",
    "model.fit(df_train[\"title\"].to_numpy(), labels, epochs=10)\n",
    "\n",
    "# serialize it\n",
    "tf.saved_model.save(model, os.environ[\"AIP_MODEL_DIR\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fbf9384-bc33-4f2e-823e-4bb075c5a5d4",
   "metadata": {},
   "source": [
    "The below fires off the Vertex AI training job. Also the train, test, validation split BQ tables occur here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d149c1c1-908a-41a9-b509-c022fe7f2112",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.utils.source_utils:Training script copied to:\n",
      "gs://tmp-hackweek2021-model-training/aiplatform-2021-09-30-23:42:57.781-aiplatform_custom_trainer_script-0.1.tar.gz.\n",
      "INFO:google.cloud.aiplatform.training_jobs:Training Output directory:\n",
      "gs://tmp-hackweek2021-model-training/aiplatform-custom-training-2021-09-30-23:42:57.906 \n",
      "INFO:google.cloud.aiplatform.training_jobs:No dataset split provided. The service will use a default split.\n",
      "INFO:google.cloud.aiplatform.training_jobs:View Training:\n",
      "https://console.cloud.google.com/ai/platform/locations/us-central1/training/2615300556851249152?project=424090863877\n",
      "INFO:google.cloud.aiplatform.training_jobs:CustomTrainingJob projects/424090863877/locations/us-central1/trainingPipelines/2615300556851249152 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n"
     ]
    }
   ],
   "source": [
    "job = aiplatform.CustomTrainingJob(\n",
    "    display_name=JOB_NAME,\n",
    "    script_path=\"task_search_v1.py\",\n",
    "    container_uri=TRAIN_IMAGE,\n",
    "    requirements=[\"google-cloud-bigquery>=2.20.0\"],\n",
    "    model_serving_container_image_uri=DEPLOY_IMAGE,\n",
    ")\n",
    "\n",
    "MODEL_DISPLAY_NAME = \"ccd-search-v1-\" + TIMESTAMP\n",
    "\n",
    "# Start the training\n",
    "if TRAIN_GPU:\n",
    "    model = job.run(\n",
    "        dataset=dataset,\n",
    "        model_display_name=MODEL_DISPLAY_NAME,\n",
    "        bigquery_destination=f\"bq://{PROJECT_ID}\",\n",
    "        args=CMDARGS,\n",
    "        replica_count=1,\n",
    "        machine_type=TRAIN_COMPUTE,\n",
    "        accelerator_type=TRAIN_GPU.name,\n",
    "        accelerator_count=TRAIN_NGPU,\n",
    "    )\n",
    "else:\n",
    "    model = job.run(\n",
    "        dataset=dataset,\n",
    "        model_display_name=MODEL_DISPLAY_NAME,\n",
    "        bigquery_destination=f\"bq://{PROJECT_ID}\",\n",
    "        args=CMDARGS,\n",
    "        replica_count=1,\n",
    "        machine_type=TRAIN_COMPUTE,\n",
    "        accelerator_count=0,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb91b9d-1b7d-4805-8518-479d6aca2bc5",
   "metadata": {},
   "source": [
    "## Deploy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac47416-f643-4cee-a79b-9acdd2e71dac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.models:Creating Endpoint\n",
      "INFO:google.cloud.aiplatform.models:Create Endpoint backing LRO: projects/424090863877/locations/us-central1/endpoints/257250536527495168/operations/5320569477278990336\n",
      "INFO:google.cloud.aiplatform.models:Endpoint created. Resource name: projects/424090863877/locations/us-central1/endpoints/257250536527495168\n",
      "INFO:google.cloud.aiplatform.models:To use this Endpoint in another session:\n",
      "INFO:google.cloud.aiplatform.models:endpoint = aiplatform.Endpoint('projects/424090863877/locations/us-central1/endpoints/257250536527495168')\n",
      "INFO:google.cloud.aiplatform.models:Deploying model to Endpoint : projects/424090863877/locations/us-central1/endpoints/257250536527495168\n",
      "INFO:google.cloud.aiplatform.models:Deploy Endpoint model backing LRO: projects/424090863877/locations/us-central1/endpoints/257250536527495168/operations/8765823192217419776\n"
     ]
    }
   ],
   "source": [
    "DEPLOYED_NAME = \"ccd-search-v1-deployed-\" + TIMESTAMP\n",
    "\n",
    "TRAFFIC_SPLIT = {\"0\": 100}\n",
    "\n",
    "MIN_NODES = 1\n",
    "MAX_NODES = 1\n",
    "\n",
    "if DEPLOY_GPU:\n",
    "    endpoint = model.deploy(\n",
    "        deployed_model_display_name=DEPLOYED_NAME,\n",
    "        traffic_split=TRAFFIC_SPLIT,\n",
    "        machine_type=DEPLOY_COMPUTE,\n",
    "        accelerator_type=DEPLOY_GPU.name,\n",
    "        accelerator_count=DEPLOY_NGPU,\n",
    "        min_replica_count=MIN_NODES,\n",
    "        max_replica_count=MAX_NODES,\n",
    "    )\n",
    "else:\n",
    "    endpoint = model.deploy(\n",
    "        deployed_model_display_name=DEPLOYED_NAME,\n",
    "        traffic_split=TRAFFIC_SPLIT,\n",
    "        machine_type=DEPLOY_COMPUTE,\n",
    "        accelerator_type=DEPLOY_COMPUTE.name,\n",
    "        accelerator_count=0,\n",
    "        min_replica_count=MIN_NODES,\n",
    "        max_replica_count=MAX_NODES,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "904a123a-1e28-4554-a013-5f32e0588651",
   "metadata": {},
   "source": [
    "## Predictions\n",
    "\n",
    "TBD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902e7d91-831c-41ab-826f-4adcfa6cc29c",
   "metadata": {},
   "source": [
    "## Remove Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186adc22-8c81-4cf4-9bf8-cadf830c2307",
   "metadata": {},
   "outputs": [],
   "source": [
    "deployed_model_id = endpoint.list_models()[0].id\n",
    "endpoint.undeploy(deployed_model_id=deployed_model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "594ed469-c2ce-49f8-aaa2-53306252e618",
   "metadata": {},
   "source": [
    "## Clean-up\n",
    "\n",
    "Uncomment and run to clean up all the training jobs, models, buckets, etc..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f14ae1-ebba-4f89-83dc-e75a3270a5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete_training_job = True\n",
    "# delete_model = True\n",
    "# delete_endpoint = True\n",
    "\n",
    "# # Warning: Setting this to true will delete everything in your bucket\n",
    "# delete_bucket = False\n",
    "\n",
    "# # Delete the training job\n",
    "# job.delete()\n",
    "\n",
    "# # Delete the model\n",
    "# model.delete()\n",
    "\n",
    "# # Delete the endpoint\n",
    "# endpoint.delete()\n",
    "\n",
    "# if delete_bucket and \"BUCKET_NAME\" in globals():\n",
    "#     ! gsutil -m rm -r $BUCKET_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "835993db-09af-4551-b3e3-a3abf5fd57a9",
   "metadata": {},
   "source": [
    "# TODO\n",
    "\n",
    "1. Utilize deployment for prediction\n",
    "2. Set \"task.py\" filename by config param\n",
    "3. Supply train/test/validation splits fractions in training job"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-3.m80",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-3:m80"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
